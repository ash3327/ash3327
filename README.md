<h1 align="center">Hi thereüëã, I'm Sam</h1>
<h3 align="center">A Year 4 AI Enthusiast & App Developer at The Chinese University of Hong Kong üíª</h3>
<p align="center">Nice to meet you! I love creating little gadgets and dive into the possibilities of AI. </p>
<br>
<h2 align="center">üí¨ About Me</h1>

- Energetic and enthusiastic towards learning new things.
- Have experiences with PyTorch and Tensorflow projects
- Graduating with a major in AI in Summer 2025
- üèÜ Dean's List, 2021-2022, 2022-2023 and 2023-2024 in CUHK 

* üöÄ **Skills:**
  - **AI**: 
      - **Frameworks**: Python, Tensorflow, PyTorch, HuggingFace, Keras
      - **Models**: 
          - **Reinforcement Learning**: Deep Q-Learning
          - **Vision**: ResNet, U-Net, GANs, Diffusion
          - **NLP**: RNN, LSTM, BERT, LLaMa
  - **Backend**: Python, Java, MySQL, Docker, Flask
  - **Frontend**: JavaScript, CSS, HTML
  - **Planning to Learn**: Node.js, React

<!-- ## üî≠ Current Projects -->
<h2 align="center">üíº Work Experiences</h2>

## Glassbox AI (ML Intern)
Jun 2024 - Aug 2024 | Summer Internship<br>
Sep 2024 - Nov 2024 | Part-time

- Implemented and trained RNN-based models for sign language translation tasks
- Developed a backend pipeline for data fetching and LLM inference, integrating it with existing fine-tuning workflows using Python, Flask, and MySQL.
- Researched methods for temporal alignment on gesture sequences

<h2 align="center">üöÄ Highlighted Projects</h2>

<table align="center">
  <tr>
    <th align="center">Oasis: The Calendar App (Jun 2024)</th>
    <th align="center">SnowFight: Deep Q-Learning Agent for Third-Person Shooter Game (Dec 2022)</th>
    <th align="center">RegSubjer:<br>Course Registration with Autoclicker (Jan 2022)</th>
  </tr>
  <tr>
    <td align="center">
      <a href="https://github.com/ash3327/OasisPlanner/tree/development" target="_blank">
        <img src="https://github.com/user-attachments/assets/a323a8c3-024d-4921-9226-ca3056a0b15e" width=250 height=250/>
      </a>
    </td>
    <td align="center">
      <a href="https://github.com/ash3327/SnowFight" target="_blank">
        <img src="https://github.com/ash3327/ash3327/assets/86100752/60f36fa1-d6fd-490b-b275-19bb1cbe9715" width=250 height=250/>
      </a>
    </td>
    <td align="center">
      <a href="https://github.com/ash3327/RegSubjer" target="_blank">
        <img src="https://github.com/user-attachments/assets/8baf9705-df8c-4380-9f41-b30560529711" width=250 height=250/>
      </a>
    </td>
  </tr>
  <tr>
    <td align="center">
      <img src="https://img.shields.io/badge/Java-ED8B00?style=flat&logo=openjdk&logoColor=white&labelColor=transparent" alt="Java" height="20"/>
      <img src="https://img.shields.io/badge/Android-3DDC84?style=flat&logo=android&logoColor=white&labelColor=transparent" alt="Android" height="20"/>
      <img src="https://img.shields.io/badge/RoomDB-005571?style=flat&logo=android&logoColor=white&labelColor=transparent" alt="RoomDB" height="20"/>
    </td>
    <td align="center">
      <img src="https://img.shields.io/badge/Python-3776AB?style=flat&logo=python&logoColor=white&labelColor=transparent"/> <img src="https://img.shields.io/badge/PyTorch-EE4C2C?style=flat&logo=pytorch&logoColor=white&labelColor=transparent"/> <img src="https://img.shields.io/badge/Deep%20Learning-F26522?style=flat&logo=deeplearning&logoColor=white&labelColor=transparent"/>
    </td>
    <td align="center">
      <img src="https://img.shields.io/badge/Java-ED8B00?style=flat&logo=openjdk&logoColor=white&labelColor=transparent"/> <img src="https://img.shields.io/badge/NTPUDPClient-005571?style=flat&logo=java&logoColor=white&labelColor=transparent"/>
    </td>
  </tr>
</table>

<!-- <div align="center">
  <a href="https://github.com/ash3327?tab=repositories&sort=stargazers" target="_blank">
    <img src="https://img.shields.io/badge/View%20All%20Projects-4285F4?style=for-the-badge&logo=github&logoColor=white" alt="View All Projects"/>
  </a>
  &nbsp;&nbsp;
  <a href="https://ash3327.github.io" target="_blank">
    <img src="https://img.shields.io/badge/Personal%20Website-FF5722?style=for-the-badge&logo=google-chrome&logoColor=white" alt="Personal Website"/>
  </a>
</div> -->

---

### Calendar App Project ‚ÄúOasis‚Äù
*Mar 2023 - Present*

* Independently developed a mobile event planning and notification application for Android using Java, aiming to help users track their deadlines and events
* Worked across the development lifecycle to build and maintain code
* Migrated the data storage and retrieval to the more robust RoomDatabase with extensive use of SQL queries
* Refactored codebase into distinct UI layers and business logic components

Others:
* Link: https://github.com/ash3327/OasisPlanner/tree/development
* Latest Release: Still in development phase.
* Skills: Java ¬∑ RoomDatabase ¬∑ SQL

### AI Music Project
*Sep 2024 - Dec 2024*

* Gesture detection and mapping into the 3d scene with three.js
* Beatmap generation via librosa

https://github.com/ash3327/ai_music_project

### Peer-to-Peer Communication App
*Jan 2024 - Apr 2024*

https://github.com/ash3327/Peer-to-Peer-Communication-App

* Created a peer-to-peer communication app supporting audio recording, waveform display and editing, and also screen share function
* Implemented GUI for audio recording, waveform display and other functions.
* Implemented synchronization mechanism for audio and video packets sent through socket.
* Skills: Python

### Archaic RPG game
* A customizable RPG game with map creation tools.
* Developed a GamePlayer feature allowing users to play their maps or load other people's maps.
* Created a MapCreator tool for users to change landscape tiles, place items, monsters, and NPCs, and add multiple levels to their maps.
* Implemented a Custom Code Editor enabling users to customize map interactions, including NPC interactions through custom coding.
* Link: https://github.com/ash3327/ArchaicBitmapGame
* Skills: Java

## Deep Q-Learning Agent for Third-Person Shooter Game [![](https://img.shields.io/badge/Updated-Dec%202022-blue.svg)]() [![ImageSegmentation-UNet](https://img.shields.io/badge/GitHub-SnowFight-orange.svg?logo=github)](https://github.com/ash3327/SnowFight) [![Report](https://img.shields.io/badge/Report-4285F4?style=flat&logo=github&logoColor=white)](https://github.com/ash3327/SnowFight/blob/main/project%20report%20-%20group%205.pdf) ![Group Project](https://img.shields.io/badge/Group_Project-FF9900?style=flat) ![Project](https://img.shields.io/badge/Project-red.svg) 
[![Python](https://img.shields.io/badge/Python-3776AB?style=flat&logo=python&logoColor=white)](https://www.python.org/)
[![Gymnasium](https://img.shields.io/badge/Gymnasium-8B9467?style=flat&logo=openai)](https://gymnasium.farama.org/index.html)
[![Reinforcement Learning](https://img.shields.io/badge/Reinforcement_Learning-00BFFF?style=flat)]()

- Created a Gym environment of a simple third-person shooter game in Python
- Implemented a Deep-Q Network with PyTorch to train agents to master the game with a variable quantity of moving objects
- Fine-tuned the model to achieve average kill streak of 7 and lengthen survival duration by 4 times, significantly better than the random baseline.

With limited information provided to the agent, the agents developed the following strategies without prior prompting:
| Precise Shooting | Retreats to Corner | Constant Spinning |
| -- | -- | -- |
| <image src="https://github.com/ash3327/ash3327/assets/86100752/60f36fa1-d6fd-490b-b275-19bb1cbe9715" width=250 height=250 /> | <image src="docs/snowfight/results-2.gif" width=250 height=250 /> | <image src="docs/snowfight/results-3.gif" width=250 height=250 /> |

<h2 align="center">üî≠ More on Past Projects</h1>

## Project Vision Transformer [![](https://img.shields.io/badge/Updated-Apr%202024-blue.svg)]() [![ImageSegmentation-UNet](https://img.shields.io/badge/GitHub-Proj--Vision--Transformer-orange.svg?logo=github)](https://github.com/ash3327/proj-vision-transformer) [![Report](https://img.shields.io/badge/Report-4285F4?style=flat&logo=github&logoColor=white)](https://github.com/ash3327/proj-vision-transformer/blob/master/project-final-report-1155175983.pdf) ![Project](https://img.shields.io/badge/Project-red.svg) 

[![Python](https://img.shields.io/badge/Python-3776AB.svg?logo=python&logoColor=white)](https://www.python.org/) 
[![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C.svg?logo=pytorch&logoColor=white)](https://pytorch.org/) 
[![UNet](https://img.shields.io/badge/Paper-UNet-green?logo=arxiv&color=green)](https://arxiv.org/abs/1505.04597)
[![ResNet](https://img.shields.io/badge/Paper-ResNet-green?logo=arxiv&color=green)](https://arxiv.org/abs/1512.03385)
[![ViT](https://img.shields.io/badge/Paper-ViT-green?logo=arxiv&color=green)](https://arxiv.org/abs/2010.11929)
[![DeiT](https://img.shields.io/badge/Model-DeiT-orange?logo=github&color=orange)](https://github.com/facebookresearch/deit)
[![T2T](https://img.shields.io/badge/Model-T2T-orange?logo=github&color=orange)](https://github.com/yitu-opensource/T2T-ViT)
[![Dataset | CIFAR10](https://img.shields.io/badge/Dataset-CIFAR10-blue.svg)](https://www.cs.toronto.edu/~kriz/cifar.html)
[![Dataset | STL10](https://img.shields.io/badge/Dataset-STL10-blue.svg)](https://cs.stanford.edu/~acoates/stl10/)
[![Dataset | Cityscapes](https://img.shields.io/badge/Dataset-Cityscapes-blue.svg)](https://www.cityscapes-dataset.com/)

Exploring the **generalizability of Vision Transformers (ViTs)** on small datasets compared to Convolutional Neural Networks (CNNs). The project highlights:
- **Scalability**: ViTs underperform on datasets with very small sizes, while CNNs are more robust.
- **Efficiency**: ViTs are computationally less efficient than CNNs for models with the same accuracy.
- Models used: ResNet, ViT, DeiT, and T2T-ViT.  

Also touched training U-Net over the Cityscape dataset.

### Key Results  
| | <span align="center">**Accuracy Comparison**</span> | <span align="center">**Computational Efficiency**</span> |
| - | --- | --- |  
| Classification | <img src="docs/vit/image2.png" width="350"> | <img src="docs/vit/image4.png" width="350"> | 
| | <span align="center">**Training IoU Curve**</span> | <span align="center">**Segmented Results**</span> |
| Segmentation |<img src="docs/vit/model_1711376232.h5_ious.png" width="350"> | <img src="docs/vit/image8.png" width="350">|

## Protein Sequence Classification Kaggle Competition [![](https://img.shields.io/badge/Updated-Mar%202024-blue.svg)]() [![ProtTrans](https://img.shields.io/badge/GitHub-AIST4010--ASM2--Protein--Transformer-orange.svg?logo=github)](https://github.com/ash3327/aist4010-coursework-asm2-protein-transformer) [![Report](https://img.shields.io/badge/Report-4285F4?style=flat&logo=github&logoColor=white&link=https://github.com/ash3327/proj-vision-transformer/blob/master/project-final-report-1155175983.pdf)](https://github.com/ash3327/aist4010-coursework-asm2-protein-transformer/blob/main/report.pdf) ![Learning](https://img.shields.io/badge/Learning-darkgreen.svg) 

[![Python](https://img.shields.io/badge/Python-3776AB.svg?logo=python&logoColor=white)](https://www.python.org/) 
[![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C.svg?logo=pytorch&logoColor=white)](https://pytorch.org/) 
[![ProtTrans](https://img.shields.io/badge/Model-ProtTrans-green?logo=arxiv&color=green)](https://github.com/agemagician/ProtTrans)
[![Kaggle](https://img.shields.io/badge/Kaggle-Protein%20Sequence%20Classification%20Challenge-blue.svg)](https://www.kaggle.com/competitions/aist4010-spring2024-a2/leaderboard?tab=public)

* Applied pretrained ProtTrans transformer model and trained a fully-connected classifier head.
* Achieved 98.438% accuracy on public leaderboard and 94.161% accuracy on private leaderboard. CNN baseline is 77.106% and 78.241%.

## Image Segmentation using U-Net [![](https://img.shields.io/badge/Updated-Dec%202023-blue.svg)]() [![ImageSegmentation-UNet](https://img.shields.io/badge/GitHub-ImageSegmentation--UNet-orange.svg?logo=github)](https://github.com/ash3327/ImageSegmentation-UNet) ![Learning](https://img.shields.io/badge/Learning-darkgreen.svg)
[![Python](https://img.shields.io/badge/Python-3776AB.svg?logo=python&logoColor=white)](https://www.python.org/) 
[![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C.svg?logo=pytorch&logoColor=white)](https://pytorch.org/) 
![AI](https://img.shields.io/badge/AI-orange.svg) ![Segmentation](https://img.shields.io/badge/Image%20Segmentation-red.svg) 
[![Kaggle](https://img.shields.io/badge/Kaggle-Carvana%20Image%20Masking%20Challenge-blue.svg)](https://www.kaggle.com/competitions/carvana-image-masking-challenge) 
[![Cityscapes](https://img.shields.io/badge/Dataset-Cityscape%20Dataset-00BFFF.svg)](https://www.cityscapes-dataset.com/) 
[![Reference](https://img.shields.io/badge/Reference-YouTube-FF0000.svg?logo=youtube&logoColor=white&link=https://www.youtube.com/watch?v=oLvmLJkmXuc&ab_channel=AladdinPersson)](https://www.youtube.com/watch?v=oLvmLJkmXuc&ab_channel=AladdinPersson)
[![Implementation](https://img.shields.io/badge/Implementation-YouTube-FF0000.svg?logo=youtube&logoColor=white&link=https://www.youtube.com/watch?v=IHq1t7NxS8k&ab_channel=AladdinPersson)](https://www.youtube.com/watch?v=IHq1t7NxS8k&ab_channel=AladdinPersson)
[![ArXiv](https://img.shields.io/badge/ArXiv-UNet-00BFFF.svg?logo=arxiv&logoColor=white&link=https://arxiv.org/abs/1505.04597)](https://arxiv.org/abs/1505.04597)


Trained U-Net models for semantic segmentation on the **Cityscapes** and **Carvana** datasets to build basic concepts on the U-Net architecture, with the following results:
- **Carvana**: 99.55% pixel accuracy, dice score of 0.9911 over the dataset downscaled to 320 x 480
- **Cityscapes**: 84.0% pixel accuracy, did not track mIoU

<img src="docs/unet/unet_1.png" width="700">

## ObjectDetection-v1 [![](https://img.shields.io/badge/Updated-Jun%202023-blue.svg)]() [![ObjectDetection-v1](https://img.shields.io/badge/GitHub-ObjectDetection--v1-orange.svg?logo=github)](https://github.com/ash3327/ObjectDetection-v1) ![Learning](https://img.shields.io/badge/Learning-darkgreen.svg)
[![Python](https://img.shields.io/badge/Python-3776AB.svg?logo=python&logoColor=white)](https://www.python.org/) 
[![Ultralytics](https://img.shields.io/badge/Ultralytics-00875A.svg)](https://github.com/ultralytics) 
[![YOLO](https://img.shields.io/badge/YOLO-FF69B4.svg)](https://github.com/ultralytics/yolov5)
![AI](https://img.shields.io/badge/AI-orange.svg) ![Detection](https://img.shields.io/badge/Object%20Detection-EE4C2C.svg) 
[![Reference](https://img.shields.io/badge/Reference-YouTube-FF0000.svg?logo=youtube&logoColor=white)](https://www.youtube.com/watch?v=WgPbbWmnXJ8&ab_channel=Murtaza%27sWorkshop-RoboticsandAI)
[![Sort](https://img.shields.io/badge/Library-Abrewlay%20Sort-00BFFF.svg?logo=github)](https://github.com/abewley/sort)

Performed object detection and tracking on videos using **YOLOv8**. Key features include:
- Static image and video object detection
- Instance tracking with third party library and **custom algorithm** respectively
- Car counting

<table>
  <tr>
    <th>Static Image Object Detection</th>
    <th>Instance Tracking</th>
    <th>Car Counting</th>
  </tr>
  <tr>
    <td><img src="docs/yolo-1/image.png" width="250" height="250" style="object-fit: cover;"></td>
    <td><img src="docs/yolo-1/vid3.gif" width="250" height="250" style="object-fit: cover;"></td>
    <td><img src="docs/yolo-1/vid4.gif" width="250" height="250" style="object-fit: cover;"></td>
  </tr>
</table>

---

## GAN Self-Learning Project [![](https://img.shields.io/badge/Updated-Aug%202022-blue.svg)]() [![GAN-MNIST](https://img.shields.io/badge/GitHub-GAN--self--learn--v1-orange.svg?logo=github)](https://github.com/ash3327/GAN-self-learn-v1) ![Learning](https://img.shields.io/badge/Learning-darkgreen.svg)
[![Python](https://img.shields.io/badge/Python-3776AB.svg?logo=python&logoColor=white)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C.svg?logo=pytorch&logoColor=white)](https://pytorch.org/)
![Generative Adversarial Networks](https://img.shields.io/badge/GAN-Generative%20Adversarial%20Networks-blueviolet.svg)
[![MNIST Dataset](https://img.shields.io/badge/Dataset-MNIST-blue.svg?logo=arxiv&logoColor=white&link=https://arxiv.org/abs/1706.05587)](https://yann.lecun.com/exdb/mnist/) 
[![Reference](https://img.shields.io/badge/Reference-YouTube-FF0000.svg?logo=youtube&logoColor=white&link=https://www.youtube.com/playlist?list=PLhhyoLH6IjfwIp8bZnzX8QR30TRcHO8Va)](https://www.youtube.com/playlist?list=PLhhyoLH6IjfwIp8bZnzX8QR30TRcHO8Va) 
[![ArXiv](https://img.shields.io/badge/ArXiv-GAN-00BFFF.svg?logo=arxiv&logoColor=white&link=https://arxiv.org/abs/1406.2661)](https://arxiv.org/abs/1406.2661)
[![ArXiv](https://img.shields.io/badge/ArXiv-WGAN-00BFFF.svg?logo=arxiv&logoColor=white&link=https://arxiv.org/abs/1701.07875)](https://arxiv.org/abs/1701.07875)
[![ArXiv](https://img.shields.io/badge/ArXiv-Conditional%20GAN-00BFFF.svg?logo=arxiv&logoColor=white&link=https://arxiv.org/abs/1411.1784)](https://arxiv.org/abs/1411.1784)

A deep learning project exploring Generative Adversarial Networks (GANs) using the MNIST dataset:

- Experimented with different learning rates
- Observed the phoenomenon of mode collapse and the sensitivity of the GAN architecture to the learning rate
- Understanding the architecture of GAN, improvements made by WGAN, and also the principles of providing class conditions to GANs

<table>
  <tr>
    <th>Vanilla GAN</th>
    <th>WGAN</th>
  </tr>
  <tr>
    <td><img src="docs/gan/v4.gif" width="250" height="250" style="object-fit: cover;"></td>
    <td><img src="docs/gan/v5.gif" width="250" height="250" style="object-fit: cover;"></td>